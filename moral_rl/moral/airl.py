import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from envs.gym_wrapper import *
import math

# Use GPU if available
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


class DiscriminatorMLP(nn.Module):
    def __init__(self, state_shape, in_channels=6):
        super(DiscriminatorMLP, self).__init__()

        self.state_shape = state_shape
        self.in_channels = in_channels

        # Layers
        # self.action_embedding = nn.Linear(n_actions, state_shape[0]*state_shape[1])
        self.reward_l1 = nn.Linear(self.in_channels*self.state_shape[0]*self.state_shape[1], 256)
        self.reward_l2 = nn.Linear(256, 512)
        self.reward_l3 = nn.Linear(512, 256)
        self.reward_out = nn.Linear(256, 1)

        self.value_l1 = nn.Linear(self.in_channels*self.state_shape[0]*self.state_shape[1], 256)
        self.value_l2 = nn.Linear(256, 512)
        self.value_l3 = nn.Linear(512, 256)
        self.value_out = nn.Linear(256, 1)

        # Activation
        self.relu = nn.LeakyReLU(0.01)

    def g(self, state):
        state = state.view(state.shape[0], -1)

        x = self.relu(self.reward_l1(state))
        x = self.relu(self.reward_l2(x))
        x = self.relu(self.reward_l3(x))
        x = x.view(x.shape[0], -1)
        x = self.reward_out(x)

        return x

    def h(self, state):
        state = state.view(state.shape[0], -1)

        x = self.relu(self.value_l1(state))
        x = self.relu(self.value_l2(x))
        x = self.relu(self.value_l3(x))
        x = x.view(x.shape[0], -1)
        x = self.value_out(x)

        return x

    def forward(self, state, next_state, gamma):
        reward = self.g(state)
        value_state = self.h(state)
        value_next_state = self.h(next_state)
        
        #print("g = ",reward)
        #print("h = ",value_state)

        advantage = reward + gamma*value_next_state - value_state

        return advantage

    def discriminate(self, state, next_state, gamma, action_probability):
        advantage = self.forward(state, next_state, gamma)
        advantage = advantage.squeeze(1)
        exp_advantage = torch.exp(advantage)
        #print((exp_advantage/(exp_advantage + action_probability + 1e-5)).shape)

        #print(exp_advantage/(exp_advantage + action_probability))
        return exp_advantage/(exp_advantage + action_probability)

    def predict_reward(self, state, next_state, gamma, action_probability):
        advantage = self.forward(state, next_state, gamma)
        advantage = advantage.squeeze(1)

        return advantage - torch.log(action_probability)


class Discriminator(nn.Module):
    def __init__(self, state_shape, in_channels=6, latent_dim=None):
        super(Discriminator, self).__init__()

        self.state_shape = state_shape
        self.in_channels = in_channels
        self.eval = False
        self.utopia_point = None
        self.max_action_generator = None
        self.min_action_generator = None

        # estimate nadir point for action and traj with rand agent
        self.min_action_random_agent = None
        self.min_trajectory_random_agent = None

        # Latent conditioning
        if latent_dim is not None:
            self.latent_dim = latent_dim
            self.latent_embedding_value = nn.Linear(latent_dim, state_shape[0] * state_shape[1])
            self.latent_embedding_reward = nn.Linear(latent_dim, state_shape[0] * state_shape[1])
            self.in_channels = in_channels+1

        # Layers
        # self.action_embedding = nn.Linear(n_actions, state_shape[0]*state_shape[1])
        self.reward_conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=32, kernel_size=2)
        self.reward_conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2)
        self.reward_conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=2)
        self.reward_out = nn.Linear(16*(state_shape[0]-3)*(state_shape[1]-3), 1)

        self.value_conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=32, kernel_size=2)
        self.value_conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2)
        self.value_conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=2)
        self.value_out = nn.Linear(16*(state_shape[0]-3)*(state_shape[1]-3), 1)

        # Activation
        self.relu = nn.LeakyReLU(0.01)

    def set_eval(self):
        self.eval = True

    # approx of the reward function
    def g(self, state, latent=None):
        state = state.view(-1, self.in_channels, self.state_shape[0], self.state_shape[1])

        if latent is not None:
            latent = F.one_hot(latent.long(), self.latent_dim).float().to(device)
            latent = self.latent_embedding_reward(latent)
            latent = latent.view(-1, 1, self.state_shape[0], self.state_shape[1])
            if latent.shape[0] == 1:
                latent = latent.repeat_interleave(repeats=state.shape[0], dim=0)

            state = torch.cat([state, latent], dim=1)
        # Conv + Linear
        x = self.relu(self.reward_conv1(state))
        x = self.relu(self.reward_conv2(x))
        x = self.relu(self.reward_conv3(x))
        x = x.view(x.shape[0], -1)
        x = self.reward_out(x)

        return x

    # approx of the value function
    def h(self, state, latent=None):
        state = state.view(-1, self.in_channels, self.state_shape[0], self.state_shape[1])

        if latent is not None:
            latent = F.one_hot(latent.long(), self.latent_dim).float().to(device)
            latent = self.latent_embedding_value(latent)
            latent = latent.view(-1, 1, self.state_shape[0], self.state_shape[1])
            if latent.shape[0] == 1:
                latent = latent.repeat_interleave(repeats=state.shape[0], dim=0)

            state = torch.cat([state, latent], dim=1)
        # Conv + Linear
        x = self.relu(self.value_conv1(state))
        x = self.relu(self.value_conv2(x))
        x = self.relu(self.value_conv3(x))
        x = x.view(x.shape[0], -1)
        x = self.value_out(x)

        return x

    # def forward(self, state, next_state, gamma, latent=None):
    #     reward = self.g(state, latent)
    #     value_state = self.h(state, latent)
    #     value_next_state = self.h(next_state, latent)

    #     # f function in the Fu2018 paper : f = Q(s,a) - V(s) = (g(s)+gamma*h(s')) - h(s)
    #     # advantage = how much an action is a good or bad decision in a certain state 
    #     advantage = reward + gamma*value_next_state - value_state

    #     # pourquoi diviser en fonction du point d'utopie si eval = true ?
    #     # plus le point d'utopie est proche de 0, plus la valeur si dessous est grande ..
    #     # (et donc plus l'action qui amène de state à  nexte_state est plébicitée)            
    #     if self.eval:
    #         # print(" advantage = ", advantage)
    #         # print(" return advantage = ", advantage/np.abs(self.utopia_point))
    #         return advantage/np.abs(self.utopia_point)
    #         #return advantage
    #     else:
    #         return advantage

    def forward(self, state, next_state, gamma, eth_norm = None, latent=None):
        reward = self.g(state, latent)
        value_state = self.h(state, latent)
        value_next_state = self.h(next_state, latent)

        # f function in the Fu2018 paper : f = Q(s,a) - V(s) = (g(s)+gamma*h(s')) - h(s)
        # advantage = how much an action is a good or bad decision in a certain state 
        advantage = reward + gamma*value_next_state - value_state

        # pourquoi diviser en fonction du point d'utopie si eval = true ?
        # plus le point d'utopie est proche de 0, plus la valeur si dessous est grande ..
        # (et donc plus l'action qui amène de state à  nexte_state est plébicitée)            
        if self.eval:
            # print(" advantage = ", advantage)
            # print(" return advantage = ", advantage/np.abs(self.utopia_point))
            if eth_norm == "v0":
                return advantage/np.abs(self.utopia_point)
            elif eth_norm == "v1":
                return (advantage-self.min_action_generator)/(self.max_action_generator - self.min_action_generator)
            elif eth_norm == "v2":
                return ((advantage-self.min_action_generator)/(self.max_action_generator - self.min_action_generator))/abs(self.normalized_utopia_point)
            elif eth_norm == "v3":
            #     print("self.min_trajectory_generator = ", self.min_trajectory_generator)
            #     print("self.max_trajectory_generator = ", self.max_trajectory_generator)
            #     print("advantage = ", advantage)
            #     print("v = ",(advantage - self.min_trajectory_generator/self.traj_size)/(self.max_trajectory_generator - self.min_trajectory_generator))
                return (advantage - self.min_trajectory_generator/self.traj_size)/(self.max_trajectory_generator - self.min_trajectory_generator)
            elif eth_norm == "v4":
                return advantage
            elif eth_norm == "v5":
                return (advantage - self.min_trajectory_generator/self.traj_size)/(self.utopia_point - self.min_trajectory_generator)
            elif eth_norm == "v6":
                return (advantage - self.min_trajectory_random_agent/self.traj_size)/(self.utopia_point - self.min_trajectory_random_agent)
            elif eth_norm == "v7":
                return (advantage - self.mean_trajectory_random_agent/self.traj_size)/(self.utopia_point - self.mean_trajectory_random_agent)
        else:
            return advantage

    def discriminate(self, state, next_state, gamma, action_probability, latent=None):
        if latent is not None:
            advantage = self.forward(state, next_state, gamma, latent)
        else:
            advantage = self.forward(state, next_state, gamma)
        advantage = advantage.squeeze(1)
        exp_advantage = torch.exp(advantage)
        #print((exp_advantage/(exp_advantage + action_probability + 1e-5)).shape)

        #print(exp_advantage/(exp_advantage + action_probability))
        return exp_advantage/(exp_advantage + action_probability)

    def predict_reward(self, state, next_state, gamma, action_probability, latent=None):
        if latent is not None:
            advantage = self.forward(state, next_state, gamma, latent)
        else:
            advantage = self.forward(state, next_state, gamma)

        advantage = advantage.squeeze(1)

        return advantage - torch.log(action_probability)

    def predict_reward_2(self, state, next_state, gamma, action_probability, latent=None):
        if latent is not None:
            advantage = self.forward(state, next_state, gamma, latent)
        else:
            advantage = self.forward(state, next_state, gamma)

        advantage = advantage.squeeze(1)

        return advantage, advantage - torch.log(action_probability)

    def estimate_utopia(self, imitation_policy, config, steps=10000):
        env = GymWrapper(config.env_id)
        states = env.reset()
        states_tensor = torch.tensor(states).float().to(device)

        # Fetch Shapes
        n_actions = env.action_space.n
        obs_shape = env.observation_space.shape
        state_shape = obs_shape[:-1]
        in_channels = obs_shape[-1]

        # Init returns
        estimated_returns = []
        running_returns = 0

        for t in range(steps):
            actions, log_probs = imitation_policy.act(states_tensor)
            next_states, rewards, done, info = env.step(actions)

            airl_state = torch.tensor(states).to(device).float()
            airl_next_state = torch.tensor(next_states).to(device).float()
            airl_rewards = self.forward(airl_state, airl_next_state, config.gamma).item()
            if done:
                airl_rewards = 0
                next_states = env.reset()
            running_returns += airl_rewards

            if done:
                estimated_returns.append(running_returns)
                running_returns = 0

            states = next_states.copy()
            states_tensor = torch.tensor(states).float().to(device)

        # l'utopia point est simplement la moyenne des rewards estimés par le discriminateur des trajectoires finies sur n pas de temps,
        # en se référant à l'imitation policy pour le choix des actions
        self.utopia_point = sum(estimated_returns)/len(estimated_returns)
        # print(" self.utopia_point = ", self.utopia_point)

        return self.utopia_point

    def estimate_utopia_all(self, imitation_policy, env_id, gamma, steps=10000):
        env = GymWrapper(env_id)
        states = env.reset()
        states_tensor = torch.tensor(states).float().to(device)

        # Fetch Shapes
        n_actions = env.action_space.n
        obs_shape = env.observation_space.shape
        state_shape = obs_shape[:-1]
        in_channels = obs_shape[-1]

        # Init returns
        estimated_returns = []
        running_returns = 0

        min_action_generator = math.inf
        max_action_generator = -math.inf
        traj_size = 1
        traj_size_not_calculated = True
        min_trajectory_generator = math.inf
        max_trajectory_generator = -math.inf
        for t in range(steps):
            actions, log_probs = imitation_policy.act(states_tensor)
            next_states, rewards, done, info = env.step(actions)

            airl_state = torch.tensor(states).to(device).float()
            airl_next_state = torch.tensor(next_states).to(device).float()
            airl_rewards = self.forward(airl_state, airl_next_state, gamma).item()
            min_action_generator = min(airl_rewards, min_action_generator)
            max_action_generator = max(airl_rewards, max_action_generator)
            if done:
                airl_rewards = 0
                next_states = env.reset()
                if traj_size_not_calculated:
                    traj_size = t
                    traj_size_not_calculated = False
            running_returns += airl_rewards

            if done:
                estimated_returns.append(running_returns)
                min_trajectory_generator = min(min_trajectory_generator, running_returns)
                max_trajectory_generator = max(max_trajectory_generator, running_returns)
                running_returns = 0
                # print("test equals 1_v1 = ", sum(estimated_returns))
                # print("test equals 1 = ", (sum(estimated_returns) - len(estimated_returns)*min(estimated_returns))/(len(estimated_returns)*(max(estimated_returns) - min(estimated_returns))))

            states = next_states.copy()
            states_tensor = torch.tensor(states).float().to(device)

        self.traj_size = traj_size
        self.min_trajectory_generator = min_trajectory_generator
        self.max_trajectory_generator = max_trajectory_generator
        self.max_action_generator = max_action_generator
        self.min_action_generator = min_action_generator
        self.utopia_point = sum(estimated_returns)/len(estimated_returns)
        self.normalized_utopia_point = (self.utopia_point - traj_size*self.min_action_generator)/(self.max_action_generator - self.min_action_generator)


        print("utopia_point = ", self.utopia_point)
        # print("normalized_utopia_point = ", self.normalized_utopia_point)
        # print("min_trajectory_generator = ", min_trajectory_generator)
        # print("max_trajectory_generator = ", max_trajectory_generator)
        # print("min_action_generator = ", min_action_generator)
        # print("max_action_generator = ", max_action_generator)
        # print("traj_size = ", traj_size)

        # print("mean rew over 1 traj = ", estimated_returns[0])
        # print("norm v0 (div utopia_point) = ", estimated_returns[0]/abs(self.utopia_point))
        # print("norm v1 (actions values [0,1] bounded) = ", (estimated_returns[0] - traj_size*self.min_action_generator)/(self.max_action_generator - self.min_action_generator))
        # print("norm v2 (v1 / normed UP) = ", ((estimated_returns[0] - traj_size*self.min_action_generator)/(self.max_action_generator - self.min_action_generator))/abs(self.normalized_utopia_point))
        # print("norm v3 (traj values [0,1] bounded, with max_1_traj) = ", (estimated_returns[0] - self.min_trajectory_generator)/(self.max_trajectory_generator - self.min_trajectory_generator))
        # print("norm v4 (no norm) = ", estimated_returns[0])
        # print("norm v5 (traj values [0,1] bounded, with UP) = ", (estimated_returns[0] - self.min_trajectory_generator)/(self.utopia_point - self.min_trajectory_generator))
        if self.min_trajectory_random_agent != None :
            print("norm v6 (traj values [0,1] bounded, with UP and min with rand agent) = ", (estimated_returns[0] - self.min_trajectory_random_agent)/(self.utopia_point - self.min_trajectory_random_agent))

        # v_act = (estimated_returns[0]/traj_size) + 1e-1
        # # print("rew 1 act = ", v_act)
        # # print("norm v0 (div utopia_point) = ", v_act/abs(self.utopia_point))
        # # print("norm v1 (actions values [0,1] bounded) = ", (v_act - self.min_action_generator)/(self.max_action_generator - self.min_action_generator))
        # # print("norm v2 (v1 / normed UP) = ", ((v_act - self.min_action_generator)/(self.max_action_generator - self.min_action_generator))/abs(self.normalized_utopia_point))
        # # print("norm v3 (traj values [0,1] bounded, with max_1_traj) = ", (v_act - self.min_trajectory_generator)/(self.max_trajectory_generator - self.min_trajectory_generator))
        # # print("norm v4 (no norm) = ", v_act)
        # # print("norm v5 (traj values [0,1] bounded, with UP) = ", (v_act - self.min_trajectory_generator)/(self.utopia_point - self.min_trajectory_generator))
        # if self.min_trajectory_random_agent != None :
        #     print("norm v6 (traj values [0,1] bounded, with UP and min with rand agent) = ", (v_act - self.min_trajectory_random_agent)/(self.utopia_point - self.min_trajectory_random_agent))



    def estimate_nadir_point(self, rand_agent, env_id, gamma, steps=10000):
        env = GymWrapper(env_id)
        states = env.reset()
        states_tensor = torch.tensor(states).float().to(device)

        # Fetch Shapes
        n_actions = env.action_space.n
        obs_shape = env.observation_space.shape
        state_shape = obs_shape[:-1]
        in_channels = obs_shape[-1]

        # Init returns
        running_returns = 0

        min_action_random_agent = math.inf
        min_trajectory_random_agent = math.inf
        mean_trajectory_random_agent = []
        for t in range(steps):
            actions, log_probs = rand_agent.act(states_tensor)
            next_states, rewards, done, info = env.step(actions)

            airl_state = torch.tensor(states).to(device).float()
            airl_next_state = torch.tensor(next_states).to(device).float()
            airl_rewards = self.forward(airl_state, airl_next_state, gamma).item()
            min_action_random_agent = min(airl_rewards, min_action_random_agent)
            if done:
                airl_rewards = 0
                next_states = env.reset()
            running_returns += airl_rewards

            if done:
                min_trajectory_random_agent = min(min_trajectory_random_agent, running_returns)
                mean_trajectory_random_agent.append(running_returns)
                running_returns = 0
             
            states = next_states.copy()
            states_tensor = torch.tensor(states).float().to(device)

        self.mean_trajectory_random_agent = np.mean(mean_trajectory_random_agent)
        self.min_trajectory_random_agent = min_trajectory_random_agent
        self.min_action_random_agent = min_action_random_agent

        # print("min_action_random_agent = ", self.min_action_random_agent)
        print("min_trajectory_random_agent = ", self.min_trajectory_random_agent)
        print("mean_trajectory_random_agent = ", self.mean_trajectory_random_agent)
        return self.min_action_random_agent, self.min_trajectory_random_agent

    def estimate_normalisation_points(self, eth_norm, rand_agent, imitation_policy, env_id, gamma, steps=10000):
        if eth_norm == "v6":
            self.estimate_nadir_point(rand_agent, env_id, gamma, steps)
        elif eth_norm == "v7":
            self.estimate_nadir_point(rand_agent, env_id, gamma, steps)
        self.estimate_utopia_all(imitation_policy, env_id, gamma, steps)


def training_sampler(expert_trajectories, policy_trajectories, ppo, batch_size, latent_posterior=None):
    states = []
    action_probabilities = []
    next_states = []
    labels = []
    latents = []
    for i in range(batch_size):
        # 1 if (s,a,s') comes from expert, 0 otherwise
        # expert_boolean = np.random.randint(2)
        expert_boolean = 1 if i < batch_size/2 else 0
        if expert_boolean == 1:
            selected_trajectories = expert_trajectories
        else:
            selected_trajectories = policy_trajectories

        random_tau_idx = np.random.randint(len(selected_trajectories))
        random_tau = selected_trajectories[random_tau_idx]['states']
        random_state_idx = np.random.randint(len(random_tau)-1)
        state = random_tau[random_state_idx]
        next_state = random_tau[random_state_idx+1]

        # Sample random latent to condition ppo on for expert samples
        if latent_posterior is not None:
            if expert_boolean == 1:
                latent = latent_posterior.sample_prior()
                latent = latent.to(device)
            else:
                latent = torch.tensor(selected_trajectories[random_tau_idx]['latents']).to(device)

            action_probability, _ = ppo.forward(torch.tensor(state).float().to(device), latent)
            action_probability = action_probability.squeeze(0)
            latents.append(latent.cpu().item())
        else:
            action_probability, _ = ppo.forward(torch.tensor(state).float().to(device))
            action_probability = action_probability.squeeze(0)
        # Get the action that was actually selected in the trajectory
        selected_action = selected_trajectories[random_tau_idx]['actions'][random_state_idx]

        states.append(state)
        next_states.append(next_state)
        action_probabilities.append(action_probability[selected_action].item())
        labels.append(expert_boolean)

    return torch.tensor(np.array(states)).float().to(device), torch.tensor(np.array(next_states)).float().to(device), \
           torch.tensor(np.array(action_probabilities)).float().to(device),\
           torch.tensor(np.array(labels)).long().to(device), torch.tensor(np.array(latents)).float().to(device)


def update_discriminator(discriminator, optimizer, gamma, expert_trajectories, policy_trajectories, ppo, batch_size,
                         latent_posterior=None):
    criterion = nn.CrossEntropyLoss()
    states, next_states, action_probabilities, labels, latents\
        = training_sampler(expert_trajectories, policy_trajectories, ppo, batch_size, latent_posterior)
    if len(latents) > 0:
        advantages = discriminator.forward(states, next_states, gamma, latents)
    else:
        advantages = discriminator.forward(states, next_states, gamma) 
    # Cat advantages and log_probs to (batch_size, 2)
    class_predictions = torch.cat([torch.log(action_probabilities).unsqueeze(1), advantages], dim=1)
    # print("class_predictions = ", class_predictions.shape)
    # print("action_probabilities = ", torch.log(action_probabilities).unsqueeze(1).shape)
    # print("advantages = ", advantages.shape)
    # print("class_predictions = ", class_predictions[0])
    # Compute Loss function
    loss = criterion(class_predictions, labels)
    # Compute Accuracies
    label_predictions = torch.argmax(class_predictions, dim=1)
    # print("labels == 0 = ", labels == 0)
    # print("label_predictions = ", label_predictions)
    # print("label_predictions[labels == 0] = ", label_predictions[labels == 0])
    predicted_fake = (label_predictions[labels == 0] == 0).float()
    predicted_expert = (label_predictions[labels == 1] == 1).float()
    # print("predicted_fake = ", predicted_fake)
    # print("predicted_expert = ", predicted_expert)
    # print("pourcentage bonne prédiction data générées : ", torch.mean(predicted_fake).item())
    # print("pourcentage bonne prédiction data expertes : ", torch.mean(predicted_expert).item())

    # predicted_fake = tensor de la prediction du discriminant parmi les data issues du générateur. 
    # 1 si il a prédit que c'était générée par le générateur, 0 si il a prédit que c'était issu de l'expert

    # predicted_expert = tensor de la prediction du discriminant parmi les data issuse de l'expert. 
    # 0 si il a prédit que c'était générée par le générateur, 1 si il a prédit que c'était issu de l'expert

    # labels = 0 si data générée, 1 si data experte
    # label_predictions = 0 si data prédite comme générée, 1 si data prédite comme experte

    # POURQUOI ? pourquoi est ce que si advantage du discriminator > log(action_proba) du générateur alors c'est "prédit" comme expert ? et inversement ?
    # quel est le lien mathématique entre l'advantage du discriminant et la log proba de choisir une action chez le générateur ?

    # print(loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item(), torch.mean(predicted_fake).item(), torch.mean(predicted_expert).item()



