
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.6043], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.8556], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 1.]
discount = 1.0
discrim_advantages = tensor([-2.1046], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.1404], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9424], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([0.0235], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2451], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9424], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 0.0
discrim_advantages = tensor([-2.3094], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9424], grad_fn=<SubBackward0>)
done = True
info = {'P_pos': (2, 2), 'C_pos': [(1, 3), (2, 3)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.6816], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.9520], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)]}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [1. 0.]
discount = 1.0
discrim_advantages = tensor([-2.6931], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.0914], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 1.]
discount = 1.0
discrim_advantages = tensor([-1.9815], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.1312], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2557], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9432], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2557], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9432], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [1. 0.]
discount = 1.0
discrim_advantages = tensor([-2.1823], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.0975], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2982], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2982], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.2011], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.2011], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.2011], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.2982], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([0.2914], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 0.0
discrim_advantages = tensor([-2.2738], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.2011], grad_fn=<SubBackward0>)
done = True
info = {'P_pos': (2, 2), 'C_pos': [(3, 1), (1, 2)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.7644], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9110], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [5]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.7644], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.8528], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.7644], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.9110], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 1.]
discount = 1.0
discrim_advantages = tensor([-2.0787], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3386], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.1809], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [8]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-0.3016], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [7]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.1809], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': []}
actions picked = [0]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 1.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([1.4687], grad_fn=<SubBackward0>)
done = False
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': [], 'scrolling_everyone': {<envs.randomized_v1_test.PlayerSprite object at 0x7f1dd1dc1280>: ''}, 'scrolling__order_frame': None}
actions picked = [6]
state = tensor([[[1., 1., 1., 1., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 0., 0., 0., 1.],
         [1., 1., 1., 1., 1.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
rewards = [0. 0.]
discount = 0.0
discrim_advantages = tensor([-2.3974], grad_fn=<SqueezeBackward1>)
discrim_rewards = tensor([-1.3977], grad_fn=<SubBackward0>)
done = True
info = {'P_pos': (1, 3), 'C_pos': [(1, 2), (3, 1)], 'log_messages': [], 'scrolling_everyone': {<envs.randomized_v1_test.PlayerSprite object at 0x7f1dd1dc1280>: ''}, 'scrolling__order_frame': None}